name: python-finals-de

x-postgresql-connection-env: &pg-connect
  POSTGRESQL_APP_HOST: ${POSTGRESQL_APP_HOST}
  POSTGRESQL_APP_DB: ${POSTGRESQL_APP_DB}
  POSTGRESQL_APP_SCHEMA: ${POSTGRESQL_APP_SCHEMA}
  POSTGRESQL_APP_USER: ${POSTGRESQL_APP_USER}
  POSTGRESQL_APP_PASSWORD: ${POSTGRESQL_APP_PASSWORD}

x-mysql-connection-env: &mysql-connect
  MYSQL_APP_HOST: ${MYSQL_APP_HOST}
  MYSQL_APP_DB: ${MYSQL_APP_DB}
  MYSQL_APP_USER: ${MYSQL_APP_USER}
  MYSQL_APP_PASSWORD: ${MYSQL_APP_PASSWORD}

x-airflow-common-env: &airflow-common-env
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
  AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
  AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: ${AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK}
  AIRFLOW_UID: ${AIRFLOW_UID}
  _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME}
  _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD}
  SPARK_MASTER_URL: ${SPARK_MASTER_URL}

x-spark-common-env: &spark-common-env
  SPARK_RPC_AUTHENTICATION_ENABLED: ${SPARK_RPC_AUTHENTICATION_ENABLED}
  SPARK_RPC_ENCRYPTION_ENABLED: ${SPARK_RPC_ENCRYPTION_ENABLED}
  SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: ${SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED}
  SPARK_SSL_ENABLED: ${SPARK_SSL_ENABLED}

services:
  # RDBMS
  postgresql:
    build: ./infra/db/postgresql
    container_name: postgresql
    environment:
      <<: *pg-connect
      # Пароль от root
      POSTGRES_PASSWORD: ${POSTGRESQL_ROOT_PASSWORD}
    ports:
      # Для просмотра данных с localhost
      - "5432:5432"
    volumes:
      - postgresql-data:/var/lib/postgres/data
    healthcheck:
      # condition, на основе которого проверяем готовность СУБД
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  mysql:
    build: ./infra/db/mysql
    container_name: mysql
    environment:
      <<: *mysql-connect
      # Пароль от root
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
    ports:
      # Для просмотра данных с localhost
      - "3306:3306"
    volumes:
      - mysql-data:/var/lib/mysql
    healthcheck:
      # condition, на основе которого проверяем готовность СУБД
      test: ["CMD-SHELL", "mysqladmin ping -h localhost"]
      interval: 5s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # # ZK & Kafka
  # zookeeper:
  #   build: ./infra/messaging/zookeeper
  #   container_name: zookeeper
  #   environment:
  #     ZOOKEEPER_CLIENT_PORT: '2181'
  #   ports:
  #     - "2181:2181"
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '0.5'
  #         memory: 512M
  #     # condition, на основе которого проверяем готовность ZK
  #     test: ["CMD", "echo", "ruok", "|", "nc", "localhost", "2181"]
  #     interval: 5s
  #     timeout: 5s
  #     retries: 5

  # kafka:
  #   build: ./infra/messaging/kafka
  #   hostname: kafka
  #   container_name: kafka
  #   depends_on:
  #     zookeper:
  #       condition: service_healthy
  #   ports:
  #     - "9092:9092"
  #   environment:
  #     # kafka:29092 для _internal_ коннектов
  #     KAFKA_BROKER_ID: ${KAFKA_BROKER_ID}
  #     KAFKA_ZOOKEEPER_CONNECT: ${KAFKA_ZOOKEEPER_CONNECT}
  #     KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: ${KAFKA_LISTENER_SECURITY_PROTOCOL_MAP}
  #     KAFKA_ADVERTISED_LISTENERS: ${KAFKA_ADVERTISED_LISTENERS}
  #     KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: ${KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}
  #     KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: ${KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS}
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '0.5'
  #         memory: 512M
  #   healthcheck:
  #     # condition, на основе которого проверяем готовность Kafka
  #     test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
  #     interval: 5s
  #     timeout: 5s
  #     retries: 5

  # Spark
  spark-master:
    build: ./infra/spark/spark-master
    container_name: spark-master
    environment:
      <<: *spark-common-env
      SPARK_MODE: master
      SPARK_MASTER_WEBUI_PORT: 8081
    ports:
      - "8081:8081"
      - "7077:7077"
    volumes:
      - spark-data:/bitnami
#      - spark-apps:/opt/spark-apps
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8081"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  spark-worker:
    build: ./infra/spark/spark-worker
    container_name: spark-worker
    environment:
      <<: *spark-common-env
      SPARK_MODE: worker
      SPARK_MASTER_URL: ${SPARK_MASTER_URL}
      SPARK_WORKER_MEMORY: 500M
      SPARK_WORKER_CORES: 1
    volumes:
      - spark-data:/bitnami
#      - spark-apps:/opt/spark-apps
    depends_on:
      spark-master:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # Datagen
  pg_datagen:
    build: ./code/datagen/pg_datagen
    container_name: pg-datagen
    depends_on:
      postgresql:
        condition: service_healthy
    environment:
      << : *pg-connect
      # Параметры генерации
      PG_DATAGEN_NUM_USERS: ${PG_DATAGEN_NUM_USERS}
      PG_DATAGEN_NUM_PRODUCTS: ${PG_DATAGEN_NUM_PRODUCTS}
      PG_DATAGEN_NUM_ORDERS: ${PG_DATAGEN_NUM_ORDERS}
      PG_DATAGEN_NUM_ORDER_DETAILS: ${PG_DATAGEN_NUM_ORDER_DETAILS}
      PG_DATAGEN_NUM_CATEGORIES: ${PG_DATAGEN_NUM_CATEGORIES}
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # Airflow
  airflow-init:
    build: ./infra/airflow/init
    container_name: airflow-init
    depends_on:
      postgresql:
        condition: service_healthy
      mysql:
        condition: service_healthy
      # kafka:
      #   condition: service_healthy
      # zookeeper:
      #   condition: service_healthy
      pg_datagen:
        condition: service_completed_successfully
      spark-master:
        condition: service_healthy
    environment:
      <<: [*airflow-common-env, *pg-connect, *mysql-connect]
    volumes:
    - ./code/airflow/dags:/opt/airflow/dags
    - sqlite-airflow-data:/usr/local/airflow/db:rw
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  airflow-scheduler:
    build: ./infra/airflow/scheduler
    container_name: airflow-scheduler
    environment:
      <<: *airflow-common-env
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    volumes:
    - ./code/airflow/dags:/opt/airflow/dags
    - sqlite-airflow-data:/usr/local/airflow/db:rw
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  airflow-webserver:
    build: ./infra/airflow/webserver
    container_name: airflow-webserver
    environment:
      <<: *airflow-common-env
    ports:
      - "8080:8080"
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    volumes:
    - ./code/airflow/dags:/opt/airflow/dags
    - sqlite-airflow-data:/usr/local/airflow/db:rw
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

volumes:
  postgresql-data:
  mysql-data:
  sqlite-airflow-data:
  spark-data: